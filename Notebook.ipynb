{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mtech1/19CS60R28/susmit/DocRed_hongwang600/DocRed\n",
      "/home/mtech1/19CS60R28/susmit/DocRed_hongwang600\n"
     ]
    }
   ],
   "source": [
    "%cd /home/mtech1/19CS60R28/susmit/DocRed_hongwang600/DocRed\n",
    "import sys\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# import seaborn as sns\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from pytorch_transformers import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from models.bert import Bert\n",
    "%cd ..\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path='data'\n",
    "out_path='prepro_data'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0,1\"\n",
    "PRE_TRAINED_MODEL_NAME='bert-base-uncased'\n",
    "model_name='bert'\n",
    "BATCH_SIZE=4\n",
    "EPOCH = 10\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "\n",
    "MAX_LEN=512\n",
    "SEP='[SEP]'\n",
    "MASK = '[MASK]'\n",
    "CLS = \"[CLS]\"\n",
    "bert = Bert(BertModel, PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotated_file_name = os.path.join(in_path, 'train_annotated.json')\n",
    "dev_file_name = os.path.join(in_path, 'dev.json')\n",
    "test_file_name = os.path.join(in_path, 'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rel2id = json.load(open(os.path.join(out_path, 'rel2id.json'), \"r\"))\n",
    "id2rel = {v:u for u,v in rel2id.items()}\n",
    "json.dump(id2rel, open(os.path.join(out_path, 'id2rel.json'), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(*msg, print_=True, log_=True):\n",
    "    for i in range(0,len(msg)):\n",
    "        if(i==len(msg)-1):\n",
    "            end='\\n'\n",
    "        else:\n",
    "            end=' '\n",
    "        if print_:\n",
    "            print(msg[i],end=end)\n",
    "        if log_:\n",
    "            with open(os.path.join(os.path.join(\"log\", model_name+'.txt')), 'a+') as f_log:\n",
    "                f_log.write(str(msg[i])+end)\n",
    "                f_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_random_seeds(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_fpath)\n",
    "    except OSError as e:\n",
    "        return -1,-1,-1.0,0,model,optimizer \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return checkpoint['best_epoch_idx'], checkpoint['best_epoch_seed'], checkpoint['best_dev_acc'], checkpoint['epoch'], model, optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do add union of evidences=> done\n",
    "# for sent pad use 2\n",
    "\n",
    "def preprocess(data_file_name, max_length = 512, is_training = True, suffix=''):\n",
    "    ori_data=json.load(open(data_file_name))[0:100]\n",
    "    max_sent_count=0#maximum number of sentences in a doc across the dataset\n",
    "    list_sent_ids=[]\n",
    "    list_attention=[]#this stores attention of docs\n",
    "    list_sent_mask=[]#this will be used in the batch multliplication for getting the embeddings of each sentence\n",
    "    # (len(list_sent_ids),max_sent_count,max_length)\n",
    "    \n",
    "    labels=[]\n",
    "    i=0\n",
    "    for doc in ori_data:\n",
    "        i=i+1\n",
    "        sys.stdout.write(\"\\r%d/%d docs\"%(i,len(ori_data)))\n",
    "        # this dict is used to take care of multiple relations with same head and tail\n",
    "        head_tail_index={}\n",
    "        max_sent_count=max(max_sent_count,len(doc['sents']))\n",
    "        for label in doc['labels']:\n",
    "            idx_list=[]\n",
    "            head=doc['vertexSet'][label['h']]\n",
    "            tail=doc['vertexSet'][label['t']]\n",
    "            if (label['h'],label['t']) in head_tail_index:\n",
    "                labels[head_tail_index[(label['h'],label['t'])]]+=label['evidence']\n",
    "                continue\n",
    "            else:\n",
    "                head_tail_index[(label['h'],label['t'])]=len(list_sent_ids)\n",
    "            for entity in head:\n",
    "                if (entity['sent_id'],entity['pos'][0],'[unused0]') not in idx_list:\n",
    "                    idx_list.append((entity['sent_id'],entity['pos'][0],'[unused0]'))\n",
    "                if (entity['sent_id'],entity['pos'][1]+1,'[unused1]') not in idx_list:\n",
    "                    idx_list.append((entity['sent_id'],entity['pos'][1]+1,'[unused1]'))\n",
    "            for entity in tail:\n",
    "                if (entity['sent_id'],entity['pos'][0],'[unused2]') not in idx_list:\n",
    "                    idx_list.append((entity['sent_id'],entity['pos'][0],'[unused2]'))\n",
    "                if (entity['sent_id'],entity['pos'][1]+1,'[unused3]') not in idx_list:\n",
    "                    idx_list.append((entity['sent_id'],entity['pos'][1]+1,'[unused3]'))\n",
    "            idx_list.sort(key=lambda tup:(tup[0],tup[1]),reverse=True)\n",
    "            temp_doc=copy.deepcopy(doc)\n",
    "            for loc in idx_list:\n",
    "                temp_doc['sents'][loc[0]].insert(loc[1],loc[2])\n",
    "\n",
    "            sent_combine=[]\n",
    "            for sent in temp_doc['sents']:\n",
    "                sent_combine=sent_combine+sent\n",
    "            sent_ids,sent_attention_mask,sent_start_ids=bert.subword_tokenize_to_ids(sent_combine)\n",
    "            list_sent_ids.append(sent_ids[0])\n",
    "            list_attention.append(sent_attention_mask[0])\n",
    "            labels.append(label['evidence'])\n",
    "            \n",
    "            \n",
    "            sent_mask=[]\n",
    "            l=1# we start from index 1 because we skip CLS token\n",
    "            for sent in temp_doc['sents']:\n",
    "                sent_mask.append([0]*max_length)\n",
    "                j=l\n",
    "#                 print(sent)\n",
    "#                 print(\"\\n\")\n",
    "                while(j<min(max_length-2,l+len(sent))):\n",
    "                    sent_mask[-1][j]=1\n",
    "                    j+=1\n",
    "                l+=len(sent)\n",
    "                if(l>=max_length-2):\n",
    "                    break\n",
    "            list_sent_mask.append(sent_mask)\n",
    "            \n",
    "    logging('')\n",
    "    evi_labels = np.zeros((len(labels),max_sent_count),dtype = np.int64)\n",
    "    for i in range(len(labels)):\n",
    "        evi_labels[i][labels[i]]=1 #if evidence present then 1\n",
    "    print(\"max_sent_cout\",max_sent_count)\n",
    "    for i in range(len(list_sent_mask)):\n",
    "        # the label for pad sentence is 2\n",
    "        evi_labels[i][len(list_sent_mask[i]):max_sent_count]=2\n",
    "        # to pad sentences with arrays of 1s\n",
    "        list_sent_mask[i]=list_sent_mask[i]+[[1]*max_length]*(max_sent_count-len(list_sent_mask[i]))\n",
    "    list_sent_ids=np.asarray(list_sent_ids,dtype=np.int64)\n",
    "    list_attention=np.asarray(list_attention,dtype=np.int64)\n",
    "    list_sent_mask=np.asarray(list_sent_mask,dtype=np.int64)\n",
    "    \n",
    "    logging(\"Started saving\")\n",
    "    \n",
    "    logging(\"Number of instances: {}\".format(list_sent_ids.shape[0]))\n",
    "    np.save(os.path.join(out_path,suffix+'_sent_ids.npy'),list_sent_ids)\n",
    "    np.save(os.path.join(out_path,suffix+'_sent_attention.npy'),list_attention)\n",
    "    np.save(os.path.join(out_path,suffix+'_sent_mask.npy'),list_sent_mask)\n",
    "    np.save(os.path.join(out_path,suffix+'_evidence_labels.npy'),evi_labels)\n",
    "    logging(\"completed saving\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 docs\n",
      "max_sent_cout 19\n",
      "Started saving\n",
      "Number of instances: 1133\n",
      "completed saving\n",
      "\n",
      "100/100 docs\n",
      "max_sent_cout 18\n",
      "Started saving\n",
      "Number of instances: 1246\n",
      "completed saving\n",
      "\n",
      "48.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "preprocess(train_annotated_file_name, max_length = 512, is_training = False, suffix='train')\n",
    "preprocess(dev_file_name, max_length = 512, is_training = False, suffix='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Docred_dataset(Dataset):\n",
    "    def __init__(self,sent_ids,sent_attention,sent_mask,evi_target,max_len=512):\n",
    "        self.sent_ids=torch.from_numpy(sent_ids)\n",
    "        self.sent_attention=torch.from_numpy(sent_attention)\n",
    "        self.sent_mask=torch.from_numpy(sent_mask)\n",
    "        self.evi_target=torch.from_numpy(evi_target)\n",
    "        self.no_samples=evi_target.shape[0]\n",
    "    def __len__(self):\n",
    "        return evi_target.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        return {\n",
    "            'sent_ids':self.sent_ids[index],\n",
    "            'sent_attention':self.sent_attention[index],\n",
    "            'sent_mask':self.sent_mask[index],\n",
    "            'targets':self.evi_target[index]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ids=np.load(os.path.join(out_path,'train'+'_sent_ids.npy'))\n",
    "sent_attention=np.load(os.path.join(out_path,'train'+'_sent_attention.npy'))\n",
    "sent_mask=np.load(os.path.join(out_path,'train'+'_sent_mask.npy'))\n",
    "evi_target=np.load(os.path.join(out_path,'train'+'_evidence_labels.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset=Docred_dataset(sent_ids=sent_ids,sent_attention=sent_attention,sent_mask=sent_mask,evi_target=evi_target,max_len=MAX_LEN)\n",
    "dataloader=DataLoader(dataset=dataset, batch_size=BATCH_SIZE,num_workers=2)\n",
    "# def create_data_loader(max_len,batch_size):\n",
    "#     ds=Docred(sent_ids,sent_mask,evi_target,max_len=512)\n",
    "#     return DataLoader(ds,batch_size=batch_size,num_workers=4)\n",
    "# train_data_loader=create_data_loader(max_len=MAX_LEN,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_mask = (b,k,t)\n",
    "# k=max sent length\n",
    "# output =(b,t,h)\n",
    "# torch.bmm\n",
    "# (b,k,t)*(b,t,h)/(len(sent))==(b,k,h)\n",
    "# torch.sum(dim)  torch.sum(input, dim, keepdim=False, *, dtype=None) → Tensor\n",
    "\n",
    "\n",
    "class EvidenceClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EvidenceClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.dense = nn.Linear(self.bert.config.hidden_size, 3)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def forward(self, input_ids, attention_mask,sent_mask,is_training=False):\n",
    "        last_hidden_state, pooled_output = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sent_mask=sent_mask.float()\n",
    "        output=torch.bmm(sent_mask,last_hidden_state)/sent_mask.sum(axis=2)[...,None]\n",
    "        logits = self.dense(output)\n",
    "        if is_training:\n",
    "            return self.logsoftmax(logits)\n",
    "        else:\n",
    "            return self.softmax(logits)\n",
    "\n",
    "        \n",
    "#         #BMM\n",
    "#         2\n",
    "#             10->3sent\n",
    "#             5->2sent\n",
    "#             sent_mask=(2,3,10)\n",
    "#             length of each sent mask->10\n",
    "#             1st doc->1st sent(0,3)\n",
    "#             each sent_mask=[1,1,1,1,0,0,0,...]\n",
    "#             2nd doc ->(pad sent) use all ones \n",
    "#             2nd doc->3rd sent->[1,1,1,...](to divide zero issue)\n",
    "#             bert_output->(2,10,768)\n",
    "#             sent_mask*bert_output(BMM)\n",
    "#             output=(2,3,768)=(2,3,10)*(2,10,768)\n",
    "#             output(2,3,3)=Linear(output,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=EvidenceClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         loss(input(output of the model),target)\n",
    "            #no_evidence_sent=>lablel=0\n",
    "            #evidence_sent=>label=1\n",
    "            #evidence_pad=>lebel=2\n",
    "#             loss(ignore_index=2) \n",
    "#             NLLLoss\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html?highlight=nllloss#torch.nn.NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.tensor(tokenizer.encode(\"[CLS] Hello my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "# outputs = model(input_ids)\n",
    "# last_hidden_state, pooler_output = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(last_hidden_state.shape)\n",
    "# print(pooler_output.shape)\n",
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_ids,sent_attention,sent_mask,evi_target):\n",
    "    train_size=input_ids.shape[0]\n",
    "    batch_size=BATCH_SIZE\n",
    "    batch_count=int(math.ceil(train_size)/batch_size)\n",
    "    model=EvidenceClassifier()\n",
    "#     logging(model)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    criterion = nn.NLLLoss(reduction='mean',ignore_index=2)\n",
    "    optimizer = AdamW(model.parameters(),lr=1e-05,correct_bias=False)\n",
    "    \n",
    "    logging(optimizer)\n",
    "    \n",
    "    best_dev_acc = -1\n",
    "    best_epoch_idx = -1\n",
    "    best_epoch_seed = -1\n",
    "    start_epoch = 0\n",
    "    ckp_path=os.path.join('checkpoint',model_name+'_checkpoint.pt')\n",
    "    best_epoch_idx,best_epoch_seed,best_dev_acc,start_epoch,model,optimizer=load_ckp(ckp_path, model, optimizer)\n",
    "    \n",
    "    train_dataset=Docred_dataset(sent_ids=input_ids,sent_attention=sent_attention,sent_mask=sent_mask,evi_target=evi_target,max_len=MAX_LEN)\n",
    "    train_dataloader=DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,num_workers=2)\n",
    "    for epoch_idx in range(start_epoch, EPOCH):\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        logging('Epoch:', epoch_idx + 1)\n",
    "        cur_seed = RANDOM_SEED + epoch_idx + 1\n",
    "        set_random_seeds(cur_seed)\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "        train_loss_val = 0\n",
    "        is_best = False\n",
    "        \n",
    "        for i,data in enumerate(tqdm(train_dataloader)):\n",
    "            batch_sent_ids = data['sent_ids']\n",
    "            batch_sent_attention = data['sent_attention']\n",
    "            batch_sent_mask = data['sent_mask']\n",
    "            batch_evi_targets=data['targets']\n",
    "            \n",
    "            \n",
    "            # print(batch_sent_ids.shape)\n",
    "            # print(batch_sent_attention.shape)\n",
    "            # print(batch_sent_mask.shape)\n",
    "            # print(batch_evi_targets.shape)\n",
    "            if torch.cuda.is_available():\n",
    "                batch_sent_ids = batch_sent_ids.cuda()\n",
    "                batch_sent_ids = batch_sent_attention.cuda()\n",
    "                batch_sent_mask = batch_sent_mask.cuda()\n",
    "                batch_evi_targets = batch_evi_targets.cuda()\n",
    "            \n",
    "            outputs = model(batch_sent_ids,batch_sent_ids,batch_sent_mask,is_training=True)\n",
    "            loss = criterion(outputs.reshape((outputs.shape[0]*outputs.shape[1],outputs.shape[2])),batch_evi_targets.reshape((batch_evi_targets.shape[0]*batch_evi_targets.shape[1])))\n",
    "            loss.backward()\n",
    "            train_loss_val+=loss.item()\n",
    "        train_loss_val/=batch_count\n",
    "        end_time = datetime.datetime.now()\n",
    "        logging('Training_loss: ',train_loss_val)\n",
    "        logging('Time: ',end_time-start_time)\n",
    "    logging(\"*\"*50)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/284 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: False\n",
      "    eps: 1e-06\n",
      "    lr: 1e-05\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [03:38<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_loss:  1.0710816391786502\n",
      "Time:  0:03:38.994988\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [03:38<00:00,  1.30it/s]\n",
      "  0%|          | 0/284 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_loss:  1.0700294552337997\n",
      "Time:  0:03:38.840818\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [03:39<00:00,  1.30it/s]\n",
      "  0%|          | 0/284 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_loss:  1.0703184600432434\n",
      "Time:  0:03:39.096291\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [03:39<00:00,  1.30it/s]\n",
      "  0%|          | 0/284 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_loss:  1.0695986347569173\n",
      "Time:  0:03:39.226941\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [03:39<00:00,  1.30it/s]\n",
      "  0%|          | 0/284 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_loss:  1.0700902930418088\n",
      "Time:  0:03:39.125286\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [03:39<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_loss:  1.0698748620575813\n",
      "Time:  0:03:39.175527\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [03:39<00:00,  1.30it/s]\n",
      "  0%|          | 0/284 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_loss:  1.0708993658581387\n",
      "Time:  0:03:39.056320\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [03:39<00:00,  1.30it/s]\n",
      "  0%|          | 0/284 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_loss:  1.0698550920604395\n",
      "Time:  0:03:39.223055\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [03:39<00:00,  1.30it/s]\n",
      "  0%|          | 0/284 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_loss:  1.0697104593890294\n",
      "Time:  0:03:39.203251\n",
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [03:39<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_loss:  1.0694955970710243\n",
      "Time:  0:03:39.143903\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "train(input_ids=sent_ids,sent_attention=sent_attention,sent_mask = sent_mask, evi_target = evi_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import sys\n",
    "# print('A', sys.version)\n",
    "# print('B', torch.__version__)\n",
    "# print('C', torch.cuda.is_available())\n",
    "# print('D', torch.backends.cudnn.enabled)\n",
    "# device = torch.device('cuda')\n",
    "# print('E', torch.cuda.get_device_properties(device))\n",
    "# print('F', torch.tensor([1.0, 2.0]).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [ 7,  8,  9]],\n",
      "\n",
      "        [[10, 11, 12],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]]])\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12],\n",
      "        [13, 14, 15],\n",
      "        [16, 17, 18]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[[1,2,3],[4,5,6],[7,8,9]],[[10,11,12],[13,14,15],[16,17,18]]])\n",
    "print(x)\n",
    "print(x.reshape(6,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
