{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3896a2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 17 11:32:08 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K40m          Off  | 00000000:81:00.0 Off |                    0 |\r\n",
      "| N/A   34C    P8    23W / 235W |     11MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2580c598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mtech1/19CS60R28/susmit/DocRed_hongwang600/DocRed\n",
      "/home/mtech1/19CS60R28/susmit/DocRed_hongwang600\n"
     ]
    }
   ],
   "source": [
    "%cd /home/mtech1/19CS60R28/susmit/DocRed_hongwang600/DocRed\n",
    "import sys\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# import seaborn as sns\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from pytorch_transformers import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from models.bert import Bert\n",
    "%cd ..\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a6b14",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243dcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path='data'\n",
    "out_path='prepro_data'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0,1\"\n",
    "PRE_TRAINED_MODEL_NAME='bert-base-uncased'\n",
    "model_name='bert'\n",
    "BATCH_SIZE=8\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "\n",
    "MAX_LEN=512\n",
    "SEP='[SEP]'\n",
    "MASK = '[MASK]'\n",
    "CLS = \"[CLS]\"\n",
    "bert = Bert(BertModel, PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b4a45",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ddf33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07fbe3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotated_file_name = os.path.join(in_path, 'train_annotated.json')\n",
    "dev_file_name = os.path.join(in_path, 'dev.json')\n",
    "test_file_name = os.path.join(in_path, 'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb90fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rel2id = json.load(open(os.path.join(out_path, 'rel2id.json'), \"r\"))\n",
    "id2rel = {v:u for u,v in rel2id.items()}\n",
    "json.dump(id2rel, open(os.path.join(out_path, 'id2rel.json'), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a5471e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ce695d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5014ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(s, print_=True, log_=True):\n",
    "    if print_:\n",
    "        print(s)\n",
    "    if log_:\n",
    "        with open(os.path.join(os.path.join(\"log\", model_name+'.txt')), 'a+') as f_log:\n",
    "            f_log.write(str(s)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe87580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1dd46c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_random_seeds(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b3a54",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb6a970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do add union of evidences=> done\n",
    "# for sent pad use 2\n",
    "\n",
    "def preprocess(data_file_name, max_length = 512, is_training = True, suffix=''):\n",
    "    ori_data=json.load(open(data_file_name))[0:10]\n",
    "    max_sent_count=0#maximum number of sentences in a doc across the dataset\n",
    "    list_sent_ids=[]\n",
    "    list_attention=[]#this stores attention of docs\n",
    "    list_sent_mask=[]#this will be used in the batch multliplication for getting the embeddings of each sentence\n",
    "    # (len(list_sent_ids),max_sent_count,max_length)\n",
    "    \n",
    "    labels=[]\n",
    "    i=0\n",
    "    for doc in ori_data:\n",
    "        i=i+1\n",
    "        sys.stdout.write(\"\\r%d/%d docs\"%(i,len(ori_data)))\n",
    "        # this dict is used to take care of multiple relations with same head and tail\n",
    "        head_tail_index={}\n",
    "        max_sent_count=max(max_sent_count,len(doc['sents']))\n",
    "        for label in doc['labels']:\n",
    "            idx_list=[]\n",
    "            head=doc['vertexSet'][label['h']]\n",
    "            tail=doc['vertexSet'][label['t']]\n",
    "            if (label['h'],label['t']) in head_tail_index:\n",
    "                labels[head_tail_index[(label['h'],label['t'])]]+=label['evidence']\n",
    "                continue\n",
    "            else:\n",
    "                head_tail_index[(label['h'],label['t'])]=len(list_sent_ids)\n",
    "            for entity in head:\n",
    "                if (entity['sent_id'],entity['pos'][0],'[unused0]') not in idx_list:\n",
    "                    idx_list.append((entity['sent_id'],entity['pos'][0],'[unused0]'))\n",
    "                if (entity['sent_id'],entity['pos'][1]+1,'[unused1]') not in idx_list:\n",
    "                    idx_list.append((entity['sent_id'],entity['pos'][1]+1,'[unused1]'))\n",
    "            for entity in tail:\n",
    "                if (entity['sent_id'],entity['pos'][0],'[unused2]') not in idx_list:\n",
    "                    idx_list.append((entity['sent_id'],entity['pos'][0],'[unused2]'))\n",
    "                if (entity['sent_id'],entity['pos'][1]+1,'[unused3]') not in idx_list:\n",
    "                    idx_list.append((entity['sent_id'],entity['pos'][1]+1,'[unused3]'))\n",
    "            idx_list.sort(key=lambda tup:(tup[0],tup[1]),reverse=True)\n",
    "            temp_doc=copy.deepcopy(doc)\n",
    "            for loc in idx_list:\n",
    "                temp_doc['sents'][loc[0]].insert(loc[1],loc[2])\n",
    "\n",
    "            sent_combine=[]\n",
    "            for sent in temp_doc['sents']:\n",
    "                sent_combine=sent_combine+sent\n",
    "            sent_ids,sent_attention_mask,sent_start_ids=bert.subword_tokenize_to_ids(sent_combine)\n",
    "            list_sent_ids.append(sent_ids[0])\n",
    "            list_attention.append(sent_attention_mask[0])\n",
    "            labels.append(label['evidence'])\n",
    "            \n",
    "            \n",
    "            sent_mask=[]\n",
    "            l=1# we start from index 1 because we skip CLS token\n",
    "            for sent in temp_doc['sents']:\n",
    "                sent_mask.append([0]*max_length)\n",
    "                j=l\n",
    "#                 print(sent)\n",
    "#                 print(\"\\n\")\n",
    "                while(j<min(max_length-2,l+len(sent))):\n",
    "                    sent_mask[-1][j]=1\n",
    "                    j+=1\n",
    "                l+=len(sent)\n",
    "                if(l>=max_length-2):\n",
    "                    break\n",
    "            list_sent_mask.append(sent_mask)\n",
    "            \n",
    "    logging('')\n",
    "    evi_labels = np.zeros((len(labels),max_sent_count),dtype = np.int64)\n",
    "    for i in range(len(labels)):\n",
    "        evi_labels[i][labels[i]]=1 #if evidence present then 1\n",
    "    print(\"max_sent_cout\",max_sent_count)\n",
    "    for i in range(len(list_sent_mask)):\n",
    "        # the label for pad sentence is 2\n",
    "        evi_labels[i][len(list_sent_mask[i]):max_sent_count]=2\n",
    "        # to pad sentences with arrays of 1s\n",
    "        list_sent_mask[i]=list_sent_mask[i]+[[1]*max_length]*(max_sent_count-len(list_sent_mask[i]))\n",
    "    list_sent_ids=np.asarray(list_sent_ids,dtype=np.int64)\n",
    "    list_attention=np.asarray(list_attention,dtype=np.int64)\n",
    "    list_sent_mask=np.asarray(list_sent_mask,dtype=np.int64)\n",
    "    \n",
    "    logging(\"Started saving\")\n",
    "    \n",
    "    logging(\"Number of instances: {}\".format(list_sent_ids.shape[0]))\n",
    "    np.save(os.path.join(out_path,suffix+'_sent_ids.npy'),list_sent_ids)\n",
    "    np.save(os.path.join(out_path,suffix+'_sent_attention.npy'),list_attention)\n",
    "    np.save(os.path.join(out_path,suffix+'_sent_mask.npy'),list_sent_mask)\n",
    "    np.save(os.path.join(out_path,suffix+'_evidence_labels.npy'),evi_labels)\n",
    "    logging(\"completed saving\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aed4fac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 docs\n",
      "max_sent_cout 12\n",
      "Started saving\n",
      "Number of instances: 111\n",
      "completed saving\n",
      "\n",
      "10/10 docs\n",
      "max_sent_cout 13\n",
      "Started saving\n",
      "Number of instances: 161\n",
      "completed saving\n",
      "\n",
      "7.54 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "preprocess(train_annotated_file_name, max_length = 512, is_training = False, suffix='train')\n",
    "preprocess(dev_file_name, max_length = 512, is_training = False, suffix='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df36e875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab41a464",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2a6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Docred_dataset(Dataset):\n",
    "    def __init__(self,sent_ids,sent_attention,sent_mask,evi_target,max_len=512):\n",
    "        self.sent_ids=torch.from_numpy(sent_ids)\n",
    "        self.sent_attention=torch.from_numpy(sent_attention)\n",
    "        self.sent_mask=torch.from_numpy(sent_mask)\n",
    "        self.evi_target=torch.from_numpy(evi_target)\n",
    "        self.no_samples=evi_target.shape[0]\n",
    "    def __len__(self):\n",
    "        return evi_target.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        return {\n",
    "            'sent_ids':self.sent_ids[index],\n",
    "            'sent_attention':self.sent_attention[index],\n",
    "            'sent_mask':self.sent_mask[index],\n",
    "            'targets':self.evi_target[index]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8ac529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ids=np.load(os.path.join(out_path,'train'+'_sent_ids.npy'))\n",
    "sent_attention=np.load(os.path.join(out_path,'train'+'_sent_attention.npy'))\n",
    "sent_mask=np.load(os.path.join(out_path,'train'+'_sent_mask.npy'))\n",
    "evi_target=np.load(os.path.join(out_path,'train'+'_evidence_labels.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00dd8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset=Docred_dataset(sent_ids=sent_ids,sent_attention=sent_attention,sent_mask=sent_mask,evi_target=evi_target,max_len=MAX_LEN)\n",
    "dataloader=DataLoader(dataset=dataset, batch_size=BATCH_SIZE,num_workers=2)\n",
    "# def create_data_loader(max_len,batch_size):\n",
    "#     ds=Docred(sent_ids,sent_mask,evi_target,max_len=512)\n",
    "#     return DataLoader(ds,batch_size=batch_size,num_workers=4)\n",
    "# train_data_loader=create_data_loader(max_len=MAX_LEN,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e735b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55784ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbab95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda416da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "464879a6",
   "metadata": {},
   "source": [
    "### Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "354d5ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74c49b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_mask = (b,k,t)\n",
    "# k=max sent length\n",
    "# output =(b,t,h)\n",
    "# torch.bmm\n",
    "# (b,k,t)*(b,t,h)/(len(sent))==(b,k,h)\n",
    "# torch.sum(dim)  torch.sum(input, dim, keepdim=False, *, dtype=None) → Tensor\n",
    "\n",
    "\n",
    "class EvidenceClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EvidenceClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "#         self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, 3)\n",
    "    def forward(self, input_ids, attention_mask,sent_mask):\n",
    "        last_hidden_state, pooled_output = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        output=torch.BMM(sent_mask,last_hidden_state)/torch.sum(sent_mask,dim=2)\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "#         #BMM\n",
    "#         2\n",
    "#             10->3sent\n",
    "#             5->2sent\n",
    "#             sent_mask=(2,3,10)\n",
    "#             length of each sent mask->10\n",
    "#             1st doc->1st sent(0,3)\n",
    "#             each sent_mask=[1,1,1,1,0,0,0,...]\n",
    "#             2nd doc ->(pad sent) use all ones \n",
    "#             2nd doc->3rd sent->[1,1,1,...](to divide zero issue)\n",
    "#             bert_output->(2,10,768)\n",
    "#             sent_mask*bert_output(BMM)\n",
    "#             output=(2,3,768)=(2,3,10)*(2,10,768)\n",
    "#             output(2,3,3)=Linear(output,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1deeff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=EvidenceClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65445355",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 3 required positional arguments: 'input_ids', 'attention_mask', and 'sent_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c1b0b96e6a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdummy_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_instances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 3 required positional arguments: 'input_ids', 'attention_mask', and 'sent_mask'"
     ]
    }
   ],
   "source": [
    "input_ids="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc38844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         loss(input(output of the model),target)\n",
    "            #no_evidence_sent=>lablel=0\n",
    "            #evidence_sent=>label=1\n",
    "            #evidence_pad=>lebel=2\n",
    "#             loss(ignore_index=2) \n",
    "#             NLLLoss\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html?highlight=nllloss#torch.nn.NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb625340",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "393f7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(tokenizer.encode(\"[CLS] Hello my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(input_ids)\n",
    "last_hidden_state, pooler_output = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6bf75b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(last_hidden_state.shape)\n",
    "print(pooler_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4589521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000000/100000000 [00:25<00:00, 3882668.63it/s]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aefe0ab",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adcc1105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "073e2f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a71d133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
