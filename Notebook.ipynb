{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3896a2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%source` not found.\n"
     ]
    }
   ],
   "source": [
    "%source activate susmit_3_6_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2580c598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mtech1/19CS60R28/susmit/DocRed_hongwang600/DocRed\n",
      "/home/mtech1/19CS60R28/susmit/DocRed_hongwang600\n"
     ]
    }
   ],
   "source": [
    "%cd /home/mtech1/19CS60R28/susmit/DocRed_hongwang600/DocRed\n",
    "import sys\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# import seaborn as sns\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from pytorch_transformers import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from models.bert import Bert\n",
    "%cd ..\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a13ebf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/mtech1/19CS60R28/miniconda3:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "anykeystore               0.2                       <pip>\r\n",
      "apex                      0.9.10.dev0               <pip>\r\n",
      "argon2-cffi               20.1.0                    <pip>\r\n",
      "asn1crypto                0.24.0                   py36_0  \r\n",
      "async-generator           1.10                      <pip>\r\n",
      "attrs                     20.3.0                    <pip>\r\n",
      "backcall                  0.2.0                     <pip>\r\n",
      "bleach                    3.3.0                     <pip>\r\n",
      "boto3                     1.17.57                   <pip>\r\n",
      "botocore                  1.20.57                   <pip>\r\n",
      "ca-certificates           2018.03.07                    0  \r\n",
      "certifi                   2018.4.16                py36_0  \r\n",
      "cffi                      1.11.5           py36h9745a5d_0  \r\n",
      "chardet                   3.0.4            py36h0f667ec_1  \r\n",
      "click                     7.1.2                     <pip>\r\n",
      "conda                     4.5.4                    py36_0  \r\n",
      "conda-env                 2.6.0                h36134e3_1  \r\n",
      "cryptacular               1.5.5                     <pip>\r\n",
      "cryptography              2.2.2            py36h14c3975_0  \r\n",
      "cycler                    0.10.0                    <pip>\r\n",
      "dataclasses               0.8                       <pip>\r\n",
      "decorator                 5.0.3                     <pip>\r\n",
      "defusedxml                0.7.1                     <pip>\r\n",
      "Deprecated                1.2.12                    <pip>\r\n",
      "entrypoints               0.3                       <pip>\r\n",
      "filelock                  3.0.12                    <pip>\r\n",
      "gdown                     3.12.2                    <pip>\r\n",
      "greenlet                  1.0.0                     <pip>\r\n",
      "hupper                    1.10.2                    <pip>\r\n",
      "idna                      2.6              py36h82fb2a8_1  \r\n",
      "importlib-metadata        3.10.0                    <pip>\r\n",
      "ipykernel                 5.5.3                     <pip>\r\n",
      "ipython                   7.16.1                    <pip>\r\n",
      "ipython-genutils          0.2.0                     <pip>\r\n",
      "ipywidgets                7.6.3                     <pip>\r\n",
      "jedi                      0.18.0                    <pip>\r\n",
      "Jinja2                    2.11.3                    <pip>\r\n",
      "jmespath                  0.10.0                    <pip>\r\n",
      "joblib                    1.0.1                     <pip>\r\n",
      "jsonschema                3.2.0                     <pip>\r\n",
      "jupyter                   1.0.0                     <pip>\r\n",
      "jupyter-client            6.1.12                    <pip>\r\n",
      "jupyter-console           6.4.0                     <pip>\r\n",
      "jupyter-core              4.7.1                     <pip>\r\n",
      "jupyterlab-pygments       0.1.2                     <pip>\r\n",
      "jupyterlab-widgets        1.0.0                     <pip>\r\n",
      "keep                      2.10.1                    <pip>\r\n",
      "kiwisolver                1.3.1                     <pip>\r\n",
      "libedit                   3.1.20170329         h6b74fdf_2  \r\n",
      "libffi                    3.2.1                hd88cf55_4  \r\n",
      "libgcc-ng                 7.2.0                hdf63c60_3  \r\n",
      "libstdcxx-ng              7.2.0                hdf63c60_3  \r\n",
      "MarkupSafe                1.1.1                     <pip>\r\n",
      "matplotlib                3.3.4                     <pip>\r\n",
      "mistune                   0.8.4                     <pip>\r\n",
      "nbclient                  0.5.3                     <pip>\r\n",
      "nbconvert                 6.0.7                     <pip>\r\n",
      "nbformat                  5.1.3                     <pip>\r\n",
      "ncurses                   6.1                  hf484d3e_0  \r\n",
      "nest-asyncio              1.5.1                     <pip>\r\n",
      "nltk                      3.6.2                     <pip>\r\n",
      "notebook                  6.3.0                     <pip>\r\n",
      "numpy                     1.19.5                    <pip>\r\n",
      "oauthlib                  3.1.0                     <pip>\r\n",
      "openssl                   1.0.2o               h20670df_0  \r\n",
      "packaging                 20.9                      <pip>\r\n",
      "pandas                    1.1.5                     <pip>\r\n",
      "pandocfilters             1.4.3                     <pip>\r\n",
      "parso                     0.8.2                     <pip>\r\n",
      "PasteDeploy               2.1.1                     <pip>\r\n",
      "pbkdf2                    1.3                       <pip>\r\n",
      "pexpect                   4.8.0                     <pip>\r\n",
      "pickleshare               0.7.5                     <pip>\r\n",
      "Pillow                    8.2.0                     <pip>\r\n",
      "pip                       10.0.1                   py36_0  \r\n",
      "plaster                   1.0                       <pip>\r\n",
      "plaster-pastedeploy       0.7                       <pip>\r\n",
      "prometheus-client         0.10.0                    <pip>\r\n",
      "prompt-toolkit            3.0.18                    <pip>\r\n",
      "psutil                    5.8.0                     <pip>\r\n",
      "ptyprocess                0.7.0                     <pip>\r\n",
      "pycosat                   0.6.3            py36h0a5515d_0  \r\n",
      "pycparser                 2.18             py36hf9f622e_1  \r\n",
      "PyGithub                  1.55                      <pip>\r\n",
      "Pygments                  2.8.1                     <pip>\r\n",
      "PyJWT                     2.1.0                     <pip>\r\n",
      "PyNaCl                    1.4.0                     <pip>\r\n",
      "pyopenssl                 18.0.0                   py36_0  \r\n",
      "pyparsing                 2.4.7                     <pip>\r\n",
      "pyramid                   2.0                       <pip>\r\n",
      "pyramid-mailer            0.15.1                    <pip>\r\n",
      "pyrsistent                0.17.3                    <pip>\r\n",
      "pysocks                   1.6.8                    py36_0  \r\n",
      "python                    3.6.5                hc3d631a_2  \r\n",
      "python-dateutil           2.8.1                     <pip>\r\n",
      "python3-openid            3.2.0                     <pip>\r\n",
      "pytorch-transformers      1.2.0                     <pip>\r\n",
      "pytz                      2021.1                    <pip>\r\n",
      "pyzmq                     22.0.3                    <pip>\r\n",
      "qtconsole                 5.0.3                     <pip>\r\n",
      "QtPy                      1.9.0                     <pip>\r\n",
      "readline                  7.0                  ha6073c6_4  \r\n",
      "regex                     2021.4.4                  <pip>\r\n",
      "repoze.sendmail           4.4.1                     <pip>\r\n",
      "requests                  2.18.4           py36he2e5f8d_1  \r\n",
      "requests                  2.25.1                    <pip>\r\n",
      "requests-oauthlib         1.3.0                     <pip>\r\n",
      "ruamel_yaml               0.15.37          py36h14c3975_2  \r\n",
      "s3transfer                0.4.2                     <pip>\r\n",
      "sacremoses                0.0.45                    <pip>\r\n",
      "scikit-learn              0.24.1                    <pip>\r\n",
      "scipy                     1.5.4                     <pip>\r\n",
      "seaborn                   0.11.1                    <pip>\r\n",
      "Send2Trash                1.5.0                     <pip>\r\n",
      "sentencepiece             0.1.95                    <pip>\r\n",
      "setuptools                39.2.0                   py36_0  \r\n",
      "six                       1.11.0           py36h372c433_1  \r\n",
      "sklearn                   0.0                       <pip>\r\n",
      "SQLAlchemy                1.4.9                     <pip>\r\n",
      "sqlite                    3.23.1               he433501_0  \r\n",
      "terminado                 0.9.4                     <pip>\r\n",
      "terminaltables            3.1.0                     <pip>\r\n",
      "testpath                  0.4.4                     <pip>\r\n",
      "threadpoolctl             2.1.0                     <pip>\r\n",
      "tk                        8.6.7                hc745277_3  \r\n",
      "torch                     1.8.1                     <pip>\r\n",
      "tornado                   6.1                       <pip>\r\n",
      "tqdm                      4.59.0                    <pip>\r\n",
      "traitlets                 4.3.3                     <pip>\r\n",
      "transaction               3.0.1                     <pip>\r\n",
      "translationstring         1.4                       <pip>\r\n",
      "typing-extensions         3.7.4.3                   <pip>\r\n",
      "urllib3                   1.22             py36hbe7ace6_0  \r\n",
      "urllib3                   1.26.4                    <pip>\r\n",
      "velruse                   1.1.1                     <pip>\r\n",
      "venusian                  3.0.0                     <pip>\r\n",
      "wcwidth                   0.2.5                     <pip>\r\n",
      "webencodings              0.5.1                     <pip>\r\n",
      "WebOb                     1.8.7                     <pip>\r\n",
      "wheel                     0.31.1                   py36_0  \r\n",
      "widgetsnbextension        3.5.1                     <pip>\r\n",
      "wrapt                     1.12.1                    <pip>\r\n",
      "WTForms                   2.3.3                     <pip>\r\n",
      "wtforms-recaptcha         0.3.2                     <pip>\r\n",
      "xz                        5.2.4                h14c3975_4  \r\n",
      "yaml                      0.1.7                had09818_2  \r\n",
      "zipp                      3.4.1                     <pip>\r\n",
      "zlib                      1.2.11               ha838bed_2  \r\n",
      "zope.deprecation          4.4.0                     <pip>\r\n",
      "zope.interface            5.4.0                     <pip>\r\n",
      "zope.sqlalchemy           1.3                       <pip>\r\n"
     ]
    }
   ],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a6b14",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "243dcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path='data'\n",
    "out_path='prepro_data'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0,1\"\n",
    "PRE_TRAINED_MODEL_NAME='bert-base-uncased'\n",
    "model_name='bert'\n",
    "BATCH_SIZE=8\n",
    "EPOCH = 2\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "if not os.path.exists(out_path):\n",
    "    os.mkdir(out_path)\n",
    "\n",
    "MAX_LEN=512\n",
    "SEP='[SEP]'\n",
    "MASK = '[MASK]'\n",
    "CLS = \"[CLS]\"\n",
    "bert = Bert(BertModel, PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b4a45",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ddf33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07fbe3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotated_file_name = os.path.join(in_path, 'train_annotated.json')\n",
    "dev_file_name = os.path.join(in_path, 'dev.json')\n",
    "test_file_name = os.path.join(in_path, 'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb90fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rel2id = json.load(open(os.path.join(out_path, 'rel2id.json'), \"r\"))\n",
    "id2rel = {v:u for u,v in rel2id.items()}\n",
    "json.dump(id2rel, open(os.path.join(out_path, 'id2rel.json'), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5471e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ce695d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5014ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(s, print_=True, log_=True):\n",
    "    if print_:\n",
    "        print(s)\n",
    "    if log_:\n",
    "        with open(os.path.join(os.path.join(\"log\", model_name+'.txt')), 'a+') as f_log:\n",
    "            f_log.write(str(s)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe87580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1dd46c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_random_seeds(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d693fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_fpath)\n",
    "    except OSError as e:\n",
    "        return -1,-1,-1.0,0,model,optimizer \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    return checkpoint['best_epoch_idx'], checkpoint['best_epoch_seed'], checkpoint['best_dev_acc'], checkpoint['epoch'], model, optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b3a54",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb6a970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do add union of evidences=> done\n",
    "# for sent pad use 2\n",
    "\n",
    "def preprocess(data_file_name, max_length = 512, is_training = True, suffix=''):\n",
    "    ori_data=json.load(open(data_file_name))[0:10]\n",
    "    max_sent_count=0#maximum number of sentences in a doc across the dataset\n",
    "    list_sent_ids=[]\n",
    "    list_attention=[]#this stores attention of docs\n",
    "    list_sent_mask=[]#this will be used in the batch multliplication for getting the embeddings of each sentence\n",
    "    # (len(list_sent_ids),max_sent_count,max_length)\n",
    "    \n",
    "    labels=[]\n",
    "    i=0\n",
    "    for doc in ori_data:\n",
    "        i=i+1\n",
    "        sys.stdout.write(\"\\r%d/%d docs\"%(i,len(ori_data)))\n",
    "        # this dict is used to take care of multiple relations with same head and tail\n",
    "        head_tail_index={}\n",
    "        max_sent_count=max(max_sent_count,len(doc['sents']))\n",
    "        for label in doc['labels']:\n",
    "            idx_list=[]\n",
    "            head=doc['vertexSet'][label['h']]\n",
    "            tail=doc['vertexSet'][label['t']]\n",
    "            if (label['h'],label['t']) in head_tail_index:\n",
    "                labels[head_tail_index[(label['h'],label['t'])]]+=label['evidence']\n",
    "                continue\n",
    "            else:\n",
    "                head_tail_index[(label['h'],label['t'])]=len(list_sent_ids)\n",
    "            for entity in head:\n",
    "                if (entity['sent_id'],entity['pos'][0],'[unused0]') not in idx_list:\n",
    "                    idx_list.append((entity['sent_id'],entity['pos'][0],'[unused0]'))\n",
    "                if (entity['sent_id'],entity['pos'][1]+1,'[unused1]') not in idx_list:\n",
    "                    idx_list.append((entity['sent_id'],entity['pos'][1]+1,'[unused1]'))\n",
    "            for entity in tail:\n",
    "                if (entity['sent_id'],entity['pos'][0],'[unused2]') not in idx_list:\n",
    "                    idx_list.append((entity['sent_id'],entity['pos'][0],'[unused2]'))\n",
    "                if (entity['sent_id'],entity['pos'][1]+1,'[unused3]') not in idx_list:\n",
    "                    idx_list.append((entity['sent_id'],entity['pos'][1]+1,'[unused3]'))\n",
    "            idx_list.sort(key=lambda tup:(tup[0],tup[1]),reverse=True)\n",
    "            temp_doc=copy.deepcopy(doc)\n",
    "            for loc in idx_list:\n",
    "                temp_doc['sents'][loc[0]].insert(loc[1],loc[2])\n",
    "\n",
    "            sent_combine=[]\n",
    "            for sent in temp_doc['sents']:\n",
    "                sent_combine=sent_combine+sent\n",
    "            sent_ids,sent_attention_mask,sent_start_ids=bert.subword_tokenize_to_ids(sent_combine)\n",
    "            list_sent_ids.append(sent_ids[0])\n",
    "            list_attention.append(sent_attention_mask[0])\n",
    "            labels.append(label['evidence'])\n",
    "            \n",
    "            \n",
    "            sent_mask=[]\n",
    "            l=1# we start from index 1 because we skip CLS token\n",
    "            for sent in temp_doc['sents']:\n",
    "                sent_mask.append([0]*max_length)\n",
    "                j=l\n",
    "#                 print(sent)\n",
    "#                 print(\"\\n\")\n",
    "                while(j<min(max_length-2,l+len(sent))):\n",
    "                    sent_mask[-1][j]=1\n",
    "                    j+=1\n",
    "                l+=len(sent)\n",
    "                if(l>=max_length-2):\n",
    "                    break\n",
    "            list_sent_mask.append(sent_mask)\n",
    "            \n",
    "    logging('')\n",
    "    evi_labels = np.zeros((len(labels),max_sent_count),dtype = np.int64)\n",
    "    for i in range(len(labels)):\n",
    "        evi_labels[i][labels[i]]=1 #if evidence present then 1\n",
    "    print(\"max_sent_cout\",max_sent_count)\n",
    "    for i in range(len(list_sent_mask)):\n",
    "        # the label for pad sentence is 2\n",
    "        evi_labels[i][len(list_sent_mask[i]):max_sent_count]=2\n",
    "        # to pad sentences with arrays of 1s\n",
    "        list_sent_mask[i]=list_sent_mask[i]+[[1]*max_length]*(max_sent_count-len(list_sent_mask[i]))\n",
    "    list_sent_ids=np.asarray(list_sent_ids,dtype=np.int64)\n",
    "    list_attention=np.asarray(list_attention,dtype=np.int64)\n",
    "    list_sent_mask=np.asarray(list_sent_mask,dtype=np.int64)\n",
    "    \n",
    "    logging(\"Started saving\")\n",
    "    \n",
    "    logging(\"Number of instances: {}\".format(list_sent_ids.shape[0]))\n",
    "    np.save(os.path.join(out_path,suffix+'_sent_ids.npy'),list_sent_ids)\n",
    "    np.save(os.path.join(out_path,suffix+'_sent_attention.npy'),list_attention)\n",
    "    np.save(os.path.join(out_path,suffix+'_sent_mask.npy'),list_sent_mask)\n",
    "    np.save(os.path.join(out_path,suffix+'_evidence_labels.npy'),evi_labels)\n",
    "    logging(\"completed saving\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aed4fac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 docs\n",
      "max_sent_cout 12\n",
      "Started saving\n",
      "Number of instances: 111\n",
      "completed saving\n",
      "\n",
      "10/10 docs\n",
      "max_sent_cout 13\n",
      "Started saving\n",
      "Number of instances: 161\n",
      "completed saving\n",
      "\n",
      "7.1 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "preprocess(train_annotated_file_name, max_length = 512, is_training = False, suffix='train')\n",
    "preprocess(dev_file_name, max_length = 512, is_training = False, suffix='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36e875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab41a464",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb2a6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Docred_dataset(Dataset):\n",
    "    def __init__(self,sent_ids,sent_attention,sent_mask,evi_target,max_len=512):\n",
    "        self.sent_ids=torch.from_numpy(sent_ids)\n",
    "        self.sent_attention=torch.from_numpy(sent_attention)\n",
    "        self.sent_mask=torch.from_numpy(sent_mask)\n",
    "        self.evi_target=torch.from_numpy(evi_target)\n",
    "        self.no_samples=evi_target.shape[0]\n",
    "    def __len__(self):\n",
    "        return evi_target.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        return {\n",
    "            'sent_ids':self.sent_ids[index],\n",
    "            'sent_attention':self.sent_attention[index],\n",
    "            'sent_mask':self.sent_mask[index],\n",
    "            'targets':self.evi_target[index]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8ac529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ids=np.load(os.path.join(out_path,'train'+'_sent_ids.npy'))\n",
    "sent_attention=np.load(os.path.join(out_path,'train'+'_sent_attention.npy'))\n",
    "sent_mask=np.load(os.path.join(out_path,'train'+'_sent_mask.npy'))\n",
    "evi_target=np.load(os.path.join(out_path,'train'+'_evidence_labels.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00dd8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset=Docred_dataset(sent_ids=sent_ids,sent_attention=sent_attention,sent_mask=sent_mask,evi_target=evi_target,max_len=MAX_LEN)\n",
    "dataloader=DataLoader(dataset=dataset, batch_size=BATCH_SIZE,num_workers=2)\n",
    "# def create_data_loader(max_len,batch_size):\n",
    "#     ds=Docred(sent_ids,sent_mask,evi_target,max_len=512)\n",
    "#     return DataLoader(ds,batch_size=batch_size,num_workers=4)\n",
    "# train_data_loader=create_data_loader(max_len=MAX_LEN,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda416da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "464879a6",
   "metadata": {},
   "source": [
    "### Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "354d5ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74c49b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_mask = (b,k,t)\n",
    "# k=max sent length\n",
    "# output =(b,t,h)\n",
    "# torch.bmm\n",
    "# (b,k,t)*(b,t,h)/(len(sent))==(b,k,h)\n",
    "# torch.sum(dim)  torch.sum(input, dim, keepdim=False, *, dtype=None) → Tensor\n",
    "\n",
    "\n",
    "class EvidenceClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EvidenceClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.dense = nn.Linear(self.bert.config.hidden_size, 3)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def forward(self, input_ids, attention_mask,sent_mask,is_training=False):\n",
    "        print(1)\n",
    "        last_hidden_state, pooled_output = self.bert(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        print(2)\n",
    "        output=torch.BMM(sent_mask,last_hidden_state)/torch.sum(sent_mask,dim=2)\n",
    "        print(3)\n",
    "        logits = self.dense(output)\n",
    "        print(4)\n",
    "        if is_training:\n",
    "            return self.logsoftmax(logits)\n",
    "        else:\n",
    "            return self.softmax(logits)\n",
    "        print(5)\n",
    "\n",
    "        \n",
    "#         #BMM\n",
    "#         2\n",
    "#             10->3sent\n",
    "#             5->2sent\n",
    "#             sent_mask=(2,3,10)\n",
    "#             length of each sent mask->10\n",
    "#             1st doc->1st sent(0,3)\n",
    "#             each sent_mask=[1,1,1,1,0,0,0,...]\n",
    "#             2nd doc ->(pad sent) use all ones \n",
    "#             2nd doc->3rd sent->[1,1,1,...](to divide zero issue)\n",
    "#             bert_output->(2,10,768)\n",
    "#             sent_mask*bert_output(BMM)\n",
    "#             output=(2,3,768)=(2,3,10)*(2,10,768)\n",
    "#             output(2,3,3)=Linear(output,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1deeff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=EvidenceClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65445355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcc38844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         loss(input(output of the model),target)\n",
    "            #no_evidence_sent=>lablel=0\n",
    "            #evidence_sent=>label=1\n",
    "            #evidence_pad=>lebel=2\n",
    "#             loss(ignore_index=2) \n",
    "#             NLLLoss\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html?highlight=nllloss#torch.nn.NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb625340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "393f7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = torch.tensor(tokenizer.encode(\"[CLS] Hello my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "# outputs = model(input_ids)\n",
    "# last_hidden_state, pooler_output = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bf75b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(last_hidden_state.shape)\n",
    "# print(pooler_output.shape)\n",
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aefe0ab",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adcc1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_ids,sent_attention,sent_mask,evi_target):\n",
    "    train_size=input_ids.shape[0]\n",
    "    batch_size=BATCH_SIZE\n",
    "    batch_count=int(math.ceil(train_size)/batch_size)\n",
    "    model=EvidenceClassifier()\n",
    "    print(input_ids.shape)\n",
    "    print(sent_attention.shape)\n",
    "    print(sent_mask.shape)\n",
    "    print(evi_target.shape)\n",
    "    logging(model)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    criterion = nn.NLLLoss(reduction='mean')\n",
    "    optimizer = AdamW(model.parameters(),lr=1e-05,correct_bias=False)\n",
    "    \n",
    "    logging(optimizer)\n",
    "    \n",
    "    best_dev_acc = -1\n",
    "    best_epoch_idx = -1\n",
    "    best_epoch_seed = -1\n",
    "    start_epoch = 0\n",
    "    ckp_path=os.path.join('checkpoint',model_name+'_checkpoint.pt')\n",
    "    best_epoch_idx,best_epoch_seed,best_dev_acc,start_epoch,model,optimizer=load_ckp(ckp_path, model, optimizer)\n",
    "    \n",
    "    train_dataset=Docred_dataset(sent_ids=input_ids,sent_attention=sent_attention,sent_mask=sent_mask,evi_target=evi_target,max_len=MAX_LEN)\n",
    "    train_dataloader=DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,num_workers=2)\n",
    "    for epoch_idx in range(start_epoch, EPOCH):\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        logging('Epoch:', epoch_idx + 1)\n",
    "        cur_seed = RANDOM_SEED + epoch_idx + 1\n",
    "        set_random_seeds(cur_seed)\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "        train_loss_val = 0\n",
    "        is_best = False\n",
    "        \n",
    "        for i,data in enumerate((train_dataloader)):\n",
    "            batch_sent_ids = data['sent_ids']\n",
    "            batch_sent_attention = data['sent_attention']\n",
    "            batch_sent_mask = data['sent_mask']\n",
    "            batch_evi_targets=data['targets']\n",
    "            \n",
    "            \n",
    "            # print(batch_sent_ids.shape)\n",
    "            # print(batch_sent_attention.shape)\n",
    "            # print(batch_sent_mask.shape)\n",
    "            # print(batch_evi_targets.shape)\n",
    "            if torch.cuda.is_available():\n",
    "                batch_sent_ids = batch_sent_ids.cuda()\n",
    "                batch_sent_ids = batch_sent_attention.cuda()\n",
    "                batch_sent_mask = batch_sent_mask.cuda()\n",
    "                batch_evi_targets = batch_evi_targets.cuda()\n",
    "            \n",
    "            outputs = model(batch_sent_ids,batch_sent_ids,batch_sent_mask,is_training=True)\n",
    "            print(outputs.shape)\n",
    "        logging('Training_loss: ',train_loss_val)\n",
    "    logging(\"*\"*100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "073e2f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111, 512)\n",
      "(111, 512)\n",
      "(111, 12, 512)\n",
      "(111, 12)\n",
      "EvidenceClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dense): Linear(in_features=768, out_features=3, bias=True)\n",
      "  (logsoftmax): LogSoftmax(dim=-1)\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: False\n",
      "    eps: 1e-06\n",
      "    lr: 1e-05\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "Epoch:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-4639b46ef2a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msent_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_attention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msent_attention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevi_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevi_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-9be9dc5de6b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_ids, sent_attention, sent_mask, evi_target)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mbatch_evi_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_evi_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_sent_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_sent_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_sent_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training_loss: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-22c9d7c46562>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, sent_mask, is_training)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBMM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pytorch_transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, head_mask)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m             \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;31m# We create a 3D attention mask from a 2D tensor mask.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device"
     ]
    }
   ],
   "source": [
    "train(input_ids=sent_ids,sent_attention=sent_attention,sent_mask = sent_mask, evi_target = evi_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9a099fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!echo $CONDA_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15f505c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     active environment : None\n",
      "       user config file : /home/mtech1/19CS60R28/.condarc\n",
      " populated config files : \n",
      "          conda version : 4.5.4\n",
      "    conda-build version : not installed\n",
      "         python version : 3.6.5.final.0\n",
      "       base environment : /home/mtech1/19CS60R28/miniconda3  (writable)\n",
      "           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/main/noarch\n",
      "                          https://repo.anaconda.com/pkgs/free/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/free/noarch\n",
      "                          https://repo.anaconda.com/pkgs/r/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/r/noarch\n",
      "                          https://repo.anaconda.com/pkgs/pro/linux-64\n",
      "                          https://repo.anaconda.com/pkgs/pro/noarch\n",
      "          package cache : /home/mtech1/19CS60R28/miniconda3/pkgs\n",
      "                          /home/mtech1/19CS60R28/.conda/pkgs\n",
      "       envs directories : /home/mtech1/19CS60R28/miniconda3/envs\n",
      "                          /home/mtech1/19CS60R28/.conda/envs\n",
      "               platform : linux-64\n",
      "             user-agent : conda/4.5.4 requests/2.25.1 CPython/3.6.5 Linux/3.10.0-1160.15.2.el7.x86_64 centos/7 glibc/2.17\n",
      "                UID:GID : 52375:50000\n",
      "             netrc file : None\n",
      "           offline mode : False\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c5e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
